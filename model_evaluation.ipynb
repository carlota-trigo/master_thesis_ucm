{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Model Evaluation**\n",
        "\n",
        "**Course:** Master in Big Data, Data Science & AI — Master Thesis  \n",
        "**Author:** Carlota Trigo La Blanca  \n",
        "\n",
        "*This notebook will be used to evlauate the models trained, compare them, and select the best one.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\msi\\anaconda3\\envs\\tfm\\Lib\\site-packages\\h5py\\__init__.py:36: UserWarning: h5py is running against HDF5 1.14.6 when it was built against 1.14.5, this may cause problems\n",
            "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
          ]
        }
      ],
      "source": [
        "import utils\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix, roc_auc_score, \n",
        "    precision_recall_curve, average_precision_score, roc_curve,\n",
        "    matthews_corrcoef, balanced_accuracy_score,\n",
        "    precision_recall_fscore_support, f1_score\n",
        ")\n",
        "from sklearn.calibration import calibration_curve\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Theme & style\n",
        "sns.set_theme(style=\"whitegrid\")          # optional, pick your style\n",
        "\n",
        "# Colors: use Viridis everywhere\n",
        "sns.set_palette(\"viridis\")                # discrete color cycle for lines/bars\n",
        "plt.rcParams[\"image.cmap\"] = \"viridis\"    # default colormap for imshow/matshow\n",
        "\n",
        "# Font sizes (global)\n",
        "plt.rcParams.update({\n",
        "    \"axes.titlesize\": 10,       # axes titles (ax.set_title)\n",
        "    \"figure.titlesize\": 10,     # figure titles (plt.suptitle)\n",
        "    \"axes.labelsize\": 9,        # x/y axis labels\n",
        "    \"xtick.labelsize\": 8,       # tick labels\n",
        "    \"ytick.labelsize\": 8,\n",
        "    \"legend.fontsize\": 8,\n",
        "    \"legend.title_fontsize\": 9,\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration and Constants\n",
        "from pathlib import Path\n",
        "SEED = utils.SEED\n",
        "DATA_DIR = utils.DATA_DIR\n",
        "PREPARED_CSV = utils.PREPARED_CSV\n",
        "IMAGE_PATH = utils.IMAGE_PATH\n",
        "\n",
        "# Model paths - Updated to match actual model implementations\n",
        "BASELINE_DIR = Path(\"./outputs/base_model\")\n",
        "SSL_DIR = Path(\"./outputs/ssl_finetuned\")\n",
        "ENSEMBLE_DIR = Path(\"./outputs/ensemble_models\")\n",
        "INDIVIDUAL_DIR = Path(\"./outputs/individual_models\")\n",
        "\n",
        "# Model configurations - Updated with correct paths and file names\n",
        "MODELS_CONFIG = {\n",
        "    'baseline': {\n",
        "        'name': 'Baseline EfficientNetB1',\n",
        "        'path': BASELINE_DIR / \"simple_twohead_best_model.keras\",\n",
        "        'type': 'single',\n",
        "        'color': '#1f77b4'\n",
        "    },\n",
        "    'ssl': {\n",
        "        'name': 'SSL Fine-tuned',\n",
        "        'path': SSL_DIR / \"ssl_finetuned_best_model.keras\",\n",
        "        'type': 'single',\n",
        "        'color': '#ff7f0e'\n",
        "    },\n",
        "    'ensemble_voting': {\n",
        "        'name': 'Voting Ensemble',\n",
        "        'path': INDIVIDUAL_DIR,\n",
        "        'type': 'ensemble',\n",
        "        'color': '#2ca02c'\n",
        "    },\n",
        "    'ensemble_weighted': {\n",
        "        'name': 'Weighted Ensemble',\n",
        "        'path': INDIVIDUAL_DIR,\n",
        "        'type': 'ensemble', \n",
        "        'color': '#d62728'\n",
        "    }\n",
        "}\n",
        "\n",
        "# Class mappings (verified from data analysis)\n",
        "COARSE_CLASSES = ['benign', 'malignant', 'no_lesion']\n",
        "FINE_CLASSES = ['nv', 'mel', 'bkl', 'bcc', 'scc_akiec', 'vasc', 'df', 'other', 'no_lesion']\n",
        "\n",
        "# Clinical thresholds\n",
        "MALIGNANT_RECALL_THRESHOLD = 0.95  # ≥95% recall for malignant\n",
        "OOD_FPR_THRESHOLD = 0.05  # FPR@95%TPR\n",
        "\n",
        "# Bootstrap parameters\n",
        "N_BOOTSTRAP = 1000\n",
        "CONFIDENCE_LEVEL = 0.95\n",
        "\n",
        "# Use constants from utils\n",
        "IMG_SIZE = utils.IMG_SIZE\n",
        "BATCH_SIZE = utils.BATCH_SIZE\n",
        "DX_CLASSES = utils.DX_CLASSES\n",
        "LESION_TYPE_CLASSES = utils.LESION_TYPE_CLASSES\n",
        "N_DX_CLASSES = utils.N_DX_CLASSES\n",
        "N_LESION_TYPE_CLASSES = utils.N_LESION_TYPE_CLASSES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_general_metrics(labels, logits, class_names, task_name):\n",
        "    \"\"\"\n",
        "    GENERAL EVALUATION METRICS for imbalanced datasets:\n",
        "    - Macro F1: Primary summary metric (unweighted average of per-class F1)\n",
        "    - Balanced accuracy: Average recall per class\n",
        "    - MCC: Matthews Correlation Coefficient (considers full confusion matrix)\n",
        "    - Per-class precision, recall, F1: Exhaustive analysis\n",
        "    - Per-class AUPRC: Precision-Recall curves for each class\n",
        "    - ECE: Expected Calibration Error\n",
        "    \"\"\"\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "    probs = tf.nn.softmax(logits).numpy()\n",
        "    \n",
        "    # Primary metrics\n",
        "    macro_f1 = f1_score(labels, preds, average='macro', zero_division=0)\n",
        "    balanced_acc = balanced_accuracy_score(labels, preds)\n",
        "    mcc = matthews_corrcoef(labels, preds)\n",
        "    \n",
        "    # Per-class metrics\n",
        "    per_class_metrics = {}\n",
        "    auprc_scores = []\n",
        "    \n",
        "    for i, class_name in enumerate(class_names):\n",
        "        if np.any(labels == i):  # Check if class exists in labels\n",
        "            class_labels = (labels == i).astype(int)\n",
        "            class_preds = (preds == i).astype(int)\n",
        "            \n",
        "            if len(np.unique(class_labels)) > 1:  # Check if class has both positive and negative samples\n",
        "                precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "                    class_labels, class_preds, average='binary', zero_division=0\n",
        "                )\n",
        "                auroc = roc_auc_score(class_labels, probs[:, i])\n",
        "                auprc = average_precision_score(class_labels, probs[:, i])\n",
        "                \n",
        "                per_class_metrics[class_name] = {\n",
        "                    'precision': precision,\n",
        "                    'recall': recall,\n",
        "                    'f1': f1,\n",
        "                    'auroc': auroc,\n",
        "                    'auprc': auprc,\n",
        "                    'support': np.sum(class_labels)\n",
        "                }\n",
        "                auprc_scores.append(auprc)\n",
        "    \n",
        "    # AUPRC macro (average across classes)\n",
        "    auprc_macro = np.mean(auprc_scores) if auprc_scores else 0.0\n",
        "    \n",
        "    # Expected Calibration Error (ECE)\n",
        "    ece = calculate_ece(labels, probs, class_names)\n",
        "    \n",
        "    return {\n",
        "        'macro_f1': macro_f1,\n",
        "        'balanced_accuracy': balanced_acc,\n",
        "        'mcc': mcc,\n",
        "        'auprc_macro': auprc_macro,\n",
        "        'ece': ece,\n",
        "        'per_class': per_class_metrics\n",
        "    }\n",
        "\n",
        "def calculate_ece(labels, probs, class_names, n_bins=10):\n",
        "    \"\"\"\n",
        "    Classwise ECE: average over classes of bin-wise |conf - acc|.\n",
        "    Safer binning that includes probability = 1.0 in the last bin.\n",
        "    \"\"\"\n",
        "    ece_scores = []\n",
        "    for i, _ in enumerate(class_names):\n",
        "        if np.any(labels == i):\n",
        "            class_labels = (labels == i).astype(int)\n",
        "            class_probs = probs[:, i]\n",
        "            if len(np.unique(class_labels)) > 1:\n",
        "                ece = 0.0\n",
        "                for b in range(n_bins):\n",
        "                    lo = b / n_bins\n",
        "                    hi = (b + 1) / n_bins\n",
        "                    if b < n_bins - 1:\n",
        "                        in_bin = (class_probs >= lo) & (class_probs < hi)\n",
        "                    else:\n",
        "                        in_bin = (class_probs >= lo) & (class_probs <= hi)  # include 1.0\n",
        "                    if np.any(in_bin):\n",
        "                        acc = class_labels[in_bin].mean()\n",
        "                        conf = class_probs[in_bin].mean()\n",
        "                        ece += np.abs(conf - acc) * (in_bin.mean())\n",
        "                ece_scores.append(ece)\n",
        "    return float(np.mean(ece_scores)) if ece_scores else 0.0\n",
        "\n",
        "def calculate_hierarchical_metrics(predictions, coarse_classes, fine_classes):\n",
        "    \"\"\"\n",
        "    HIERARCHICAL EVALUATION (two-head architecture):\n",
        "    - Exact-match: Both heads correct simultaneously\n",
        "    - Coarse-correct: Balanced accuracy for head 1 (coarse)\n",
        "    - Fine-conditional: Macro-F1 of head 2, conditioned on head 1 being correct\n",
        "    \"\"\"\n",
        "    coarse_labels = predictions['id_labels_h1']\n",
        "    coarse_logits = predictions['id_logits_h1']\n",
        "    fine_labels = predictions['id_labels_h2']\n",
        "    fine_logits = predictions['id_logits_h2']\n",
        "    \n",
        "    coarse_preds = np.argmax(coarse_logits, axis=1)\n",
        "    fine_preds = np.argmax(fine_logits, axis=1)\n",
        "    \n",
        "    # Exact-match: both heads correct\n",
        "    exact_match = np.mean((coarse_preds == coarse_labels) & (fine_preds == fine_labels))\n",
        "    \n",
        "    # Coarse-correct: head1 balanced accuracy\n",
        "    coarse_correct = balanced_accuracy_score(coarse_labels, coarse_preds)\n",
        "    \n",
        "    # Fine-conditional: head2 macro-F1 conditioned on head1 being correct\n",
        "    coarse_correct_mask = (coarse_preds == coarse_labels)\n",
        "    if np.sum(coarse_correct_mask) > 0:\n",
        "        fine_conditional_labels = fine_labels[coarse_correct_mask]\n",
        "        fine_conditional_preds = fine_preds[coarse_correct_mask]\n",
        "        fine_conditional_f1 = f1_score(fine_conditional_labels, fine_conditional_preds, \n",
        "                                     average='macro', zero_division=0)\n",
        "    else:\n",
        "        fine_conditional_f1 = 0.0\n",
        "    \n",
        "    return {\n",
        "        'exact_match': exact_match,\n",
        "        'coarse_correct': coarse_correct,\n",
        "        'fine_conditional_f1': fine_conditional_f1\n",
        "    }\n",
        "\n",
        "def calculate_ood_metrics(id_logits, ood_logits):\n",
        "    \"\"\"\n",
        "    OUT-OF-DISTRIBUTION EVALUATION:\n",
        "    - AUROC (OOD vs ID)\n",
        "    - AUPRC (OOD as positive class)\n",
        "    - FPR@95%TPR (TPR = OOD recall)\n",
        "    - Detection Error = min_tau 0.5 * (FNR_OOD + FPR_ID)\n",
        "    \"\"\"\n",
        "    # Softmax\n",
        "    id_probs = tf.nn.softmax(id_logits, axis=1).numpy()\n",
        "    ood_probs = tf.nn.softmax(ood_logits, axis=1).numpy()\n",
        "\n",
        "    # OOD score: higher => more likely OOD (use 1 - MSP)\n",
        "    id_scores = 1.0 - np.max(id_probs, axis=1)\n",
        "    ood_scores = 1.0 - np.max(ood_probs, axis=1)\n",
        "\n",
        "    # Labels: 0 = ID, 1 = OOD\n",
        "    y = np.concatenate([np.zeros_like(id_scores), np.ones_like(ood_scores)])\n",
        "    s = np.concatenate([id_scores, ood_scores])\n",
        "\n",
        "    # AUROC / AUPRC\n",
        "    auroc = roc_auc_score(y, s)\n",
        "    auprc = average_precision_score(y, s)  # OOD is positive class\n",
        "\n",
        "    # ROC for FPR@95%TPR and Detection Error\n",
        "    fpr, tpr, thr = roc_curve(y, s)  # TPR refers to OOD (positive) recall\n",
        "    # FPR at 95% TPR (first threshold achieving >= 0.95)\n",
        "    idx = np.searchsorted(tpr, 0.95, side='left')\n",
        "    fpr_at_95_tpr = float(fpr[idx]) if idx < len(fpr) else 1.0\n",
        "\n",
        "    # Detection error (equal priors): 0.5 * (FNR_OOD + FPR_ID)\n",
        "    fnr = 1.0 - tpr\n",
        "    detection_error = float(np.min(0.5 * (fnr + fpr)))\n",
        "\n",
        "    return {\n",
        "        'auroc': auroc,\n",
        "        'auprc': auprc,\n",
        "        'fpr_at_95_tpr': fpr_at_95_tpr,\n",
        "        'detection_error': detection_error\n",
        "    }\n",
        "\n",
        "def print_metrics(coarse_metrics, fine_metrics, hierarchical_metrics, ood_metrics):\n",
        "    \"\"\"\n",
        "    Print detailed evaluation metrics in a structured format.\n",
        "    \"\"\"\n",
        "    # Print detailed results for this model\n",
        "    print(f\"\\nCOARSE CLASSIFICATION (Head 1):\")\n",
        "    print(f\"  Macro F1: {coarse_metrics['macro_f1']:.4f}\")\n",
        "    print(f\"  Balanced Accuracy: {coarse_metrics['balanced_accuracy']:.4f}\")\n",
        "    print(f\"  MCC: {coarse_metrics['mcc']:.4f}\")\n",
        "    print(f\"  AUPRC Macro: {coarse_metrics['auprc_macro']:.4f}\")\n",
        "    print(f\"  ECE: {coarse_metrics['ece']:.4f}\")\n",
        "\n",
        "    print(f\"\\nFINE CLASSIFICATION (Head 2):\")\n",
        "    print(f\"  Macro F1: {fine_metrics['macro_f1']:.4f}\")\n",
        "    print(f\"  Balanced Accuracy: {fine_metrics['balanced_accuracy']:.4f}\")\n",
        "    print(f\"  MCC: {fine_metrics['mcc']:.4f}\")\n",
        "    print(f\"  AUPRC Macro: {fine_metrics['auprc_macro']:.4f}\")\n",
        "    print(f\"  ECE: {fine_metrics['ece']:.4f}\")\n",
        "\n",
        "    print(f\"\\nHIERARCHICAL METRICS:\")\n",
        "    print(f\"  Exact Match: {hierarchical_metrics['exact_match']:.4f}\")\n",
        "    print(f\"  Coarse Correct: {hierarchical_metrics['coarse_correct']:.4f}\")\n",
        "    print(f\"  Fine Conditional F1: {hierarchical_metrics['fine_conditional_f1']:.4f}\")\n",
        "\n",
        "    print(f\"\\nOOD DETECTION:\")\n",
        "    print(f\"  AUROC: {ood_metrics['auroc']:.4f}\")\n",
        "    print(f\"  AUPRC: {ood_metrics['auprc']:.4f}\")\n",
        "    print(f\"  FPR@95%TPR: {ood_metrics['fpr_at_95_tpr']:.4f}\")\n",
        "    print(f\"  Detection Error: {ood_metrics['detection_error']:.4f}\")\n",
        "\n",
        "    # Per-class detailed analysis\n",
        "    print(f\"\\nPER-CLASS ANALYSIS - COARSE:\")\n",
        "    df = pd.DataFrame(coarse_metrics['per_class']).T\n",
        "    df = df[['precision','recall','f1','auroc','auprc','support']]\n",
        "    df['support'] = df['support'].astype(int)         # make support nice ints\n",
        "    print(df.to_string(float_format=lambda x: f\"{x:.3f}\"))\n",
        "\n",
        "    print(f\"\\nPER-CLASS ANALYSIS - FINE:\")\n",
        "    df = pd.DataFrame(fine_metrics['per_class']).T\n",
        "    df = df[['precision','recall','f1','auroc','auprc','support']]\n",
        "    df['support'] = df['support'].astype(int)         # make support nice ints\n",
        "    print(df.to_string(float_format=lambda x: f\"{x:.3f}\"))\n",
        "    \n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load models and data\n",
        "\n",
        "This code loads all the models and obtains the predictions from the test and test_ood datsets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test samples: 10359\n",
            "OOD samples: 2669\n",
            "Loading Baseline EfficientNetB1...\n",
            "✓ Loaded complete model from outputs\\base_model\\simple_twohead_best_model.keras\n",
            "✓ Baseline EfficientNetB1 loaded\n",
            "Loading SSL Fine-tuned...\n",
            "✗ Model file not found: outputs\\ssl_finetuned\\ssl_finetuned_best_model.keras\n",
            "✗ Failed to load SSL Fine-tuned\n",
            "✓ Loaded 1 individual models\n",
            "✓ Loaded 0 ensemble component models\n"
          ]
        }
      ],
      "source": [
        "def load_models():\n",
        "    \"\"\"Load all available models.\"\"\"\n",
        "    models = {}\n",
        "    ensemble_models = {}\n",
        "    \n",
        "    # Load individual models\n",
        "    for model_key, config in MODELS_CONFIG.items():\n",
        "        if config['type'] == 'single':\n",
        "            print(f\"Loading {config['name']}...\")\n",
        "            model = utils.load_individual_model(config['path'], 'efficientnet')\n",
        "            if model is not None:\n",
        "                models[model_key] = model\n",
        "                print(f\"✓ {config['name']} loaded\")\n",
        "            else:\n",
        "                print(f\"✗ Failed to load {config['name']}\")\n",
        "    \n",
        "    # Load ensemble models\n",
        "    ensemble_models = utils.load_ensemble_models(INDIVIDUAL_DIR)\n",
        "    \n",
        "    return models, ensemble_models\n",
        "\n",
        "def get_predictions_all_models(models, ensemble_models, test_ds, ood_ds):\n",
        "    \"\"\"Get predictions for all models.\"\"\"\n",
        "    all_predictions = {}\n",
        "    \n",
        "    # Individual models\n",
        "    for model_key, model in models.items():\n",
        "        print(f\"Evaluating {MODELS_CONFIG[model_key]['name']}...\")\n",
        "        try:\n",
        "            id_labels_h1, id_logits_h1, id_labels_h2, id_logits_h2 = utils.get_predictions_and_labels(model, test_ds)\n",
        "            ood_labels_h1, ood_logits_h1, ood_labels_h2, ood_logits_h2 = utils.get_predictions_and_labels(model, ood_ds)\n",
        "            \n",
        "            all_predictions[model_key] = {\n",
        "                'id_labels_h1': id_labels_h1, 'id_logits_h1': id_logits_h1,\n",
        "                'id_labels_h2': id_labels_h2, 'id_logits_h2': id_logits_h2,\n",
        "                'ood_labels_h1': ood_labels_h1, 'ood_logits_h1': ood_logits_h1,\n",
        "                'ood_labels_h2': ood_labels_h2, 'ood_logits_h2': ood_logits_h2\n",
        "            }\n",
        "            print(f\"✓ {MODELS_CONFIG[model_key]['name']} evaluated\")\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Failed to evaluate {MODELS_CONFIG[model_key]['name']}: {e}\")\n",
        "    \n",
        "    # Ensemble models\n",
        "    if len(ensemble_models) > 0:\n",
        "        print(\"Evaluating ensemble models...\")\n",
        "        try:\n",
        "            # Voting ensemble\n",
        "            id_labels_h1, id_logits_h1, id_labels_h2, id_logits_h2 = utils.get_ensemble_predictions(ensemble_models, test_ds, 'voting')\n",
        "            ood_labels_h1, ood_logits_h1, ood_labels_h2, ood_logits_h2 = utils.get_ensemble_predictions(ensemble_models, ood_ds, 'voting')\n",
        "            \n",
        "            all_predictions['ensemble_voting'] = {\n",
        "                'id_labels_h1': id_labels_h1, 'id_logits_h1': id_logits_h1,\n",
        "                'id_labels_h2': id_labels_h2, 'id_logits_h2': id_logits_h2,\n",
        "                'ood_labels_h1': ood_labels_h1, 'ood_logits_h1': ood_logits_h1,\n",
        "                'ood_labels_h2': ood_labels_h2, 'ood_logits_h2': ood_logits_h2\n",
        "            }\n",
        "            \n",
        "            # Weighted ensemble\n",
        "            id_labels_h1, id_logits_h1, id_labels_h2, id_logits_h2 = utils.get_ensemble_predictions(ensemble_models, test_ds, 'weighted')\n",
        "            ood_labels_h1, ood_logits_h1, ood_labels_h2, ood_logits_h2 = utils.get_ensemble_predictions(ensemble_models, ood_ds, 'weighted')\n",
        "            \n",
        "            all_predictions['ensemble_weighted'] = {\n",
        "                'id_labels_h1': id_labels_h1, 'id_logits_h1': id_logits_h1,\n",
        "                'id_labels_h2': id_labels_h2, 'id_logits_h2': id_logits_h2,\n",
        "                'ood_labels_h1': ood_labels_h1, 'ood_logits_h1': ood_logits_h1,\n",
        "                'ood_labels_h2': ood_labels_h2, 'ood_logits_h2': ood_logits_h2\n",
        "            }\n",
        "            \n",
        "            print(\"✓ Ensemble models evaluated\")\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Failed to evaluate ensemble models: {e}\")\n",
        "    \n",
        "    return all_predictions\n",
        "\n",
        "def get_predictions_individual_ensemble_models(ensemble_models, test_ds, ood_ds):\n",
        "    \"\"\"Get predictions for individual models in the ensemble.\"\"\"\n",
        "    individual_predictions = {}\n",
        "    \n",
        "    if len(ensemble_models) > 0:\n",
        "        print(\"Evaluating individual ensemble models...\")\n",
        "        \n",
        "        for model_name, model in ensemble_models.items():\n",
        "            print(f\"Evaluating individual model: {model_name}\")\n",
        "            try:\n",
        "                id_labels_h1, id_logits_h1, id_labels_h2, id_logits_h2 = utils.get_predictions_and_labels(model, test_ds)\n",
        "                ood_labels_h1, ood_logits_h1, ood_labels_h2, ood_logits_h2 = utils.get_predictions_and_labels(model, ood_ds)\n",
        "                \n",
        "                individual_predictions[f'individual_{model_name}'] = {\n",
        "                    'id_labels_h1': id_labels_h1, 'id_logits_h1': id_logits_h1,\n",
        "                    'id_labels_h2': id_labels_h2, 'id_logits_h2': id_logits_h2,\n",
        "                    'ood_labels_h1': ood_labels_h1, 'ood_logits_h1': ood_logits_h1,\n",
        "                    'ood_labels_h2': ood_labels_h2, 'ood_logits_h2': ood_logits_h2\n",
        "                }\n",
        "                print(f\"✓ Individual model {model_name} evaluated\")\n",
        "            except Exception as e:\n",
        "                print(f\"✗ Failed to evaluate individual model {model_name}: {e}\")\n",
        "    \n",
        "    return individual_predictions\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(PREPARED_CSV)\n",
        "test_df = df[df.split == \"test\"].copy()\n",
        "ood_df = df[df.split == \"test_ood\"].copy()\n",
        "\n",
        "print(f\"Test samples: {len(test_df)}\")\n",
        "print(f\"OOD samples: {len(ood_df)}\")\n",
        "\n",
        "# Build datasets\n",
        "test_ds = utils.build_dataset(test_df, is_training=False)\n",
        "ood_ds = utils.build_dataset(ood_df, is_training=False)\n",
        "\n",
        "models, ensemble_models = load_models()\n",
        "\n",
        "if len(models) == 0 and len(ensemble_models) == 0:\n",
        "    print(\"No models available for evaluation!\")\n",
        "else:\n",
        "    print(f\"✓ Loaded {len(models)} individual models\")\n",
        "    print(f\"✓ Loaded {len(ensemble_models)} ensemble component models\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(lesion_type\n",
              " benign       7439\n",
              " malignant    2658\n",
              " no_lesion     262\n",
              " Name: count, dtype: int64,\n",
              " diagnosis_grouped\n",
              " unknown      3606\n",
              " nv           3070\n",
              " bcc          1284\n",
              " mel           930\n",
              " bkl           615\n",
              " scc_akiec     441\n",
              " no_lesion     262\n",
              " df             66\n",
              " vasc           57\n",
              " other          28\n",
              " Name: count, dtype: int64)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df['lesion_type'].value_counts(), test_df['diagnosis_grouped'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(lesion_type\n",
              " benign    2669\n",
              " Name: count, dtype: int64,\n",
              " diagnosis_grouped\n",
              " unknown    2669\n",
              " Name: count, dtype: int64)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ood_df['lesion_type'].value_counts(), ood_df['diagnosis_grouped'].value_counts()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating Baseline EfficientNetB1...\n",
            "✓ Baseline EfficientNetB1 evaluated\n"
          ]
        }
      ],
      "source": [
        "all_predictions = get_predictions_all_models(models, ensemble_models, test_ds, ood_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "individual_ensemble_predictions = get_predictions_individual_ensemble_models(ensemble_models, test_ds, ood_ds)\n",
        "\n",
        "# Combine with main predictions\n",
        "all_predictions.update(individual_ensemble_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for key in all_predictions.keys():\n",
        "    print(f\"  - {key}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MODEL EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_results = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## EfficientNet Baseline model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "baseline_predictions = all_predictions['baseline']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Coarse classification (Head 1)\n",
        "coarse_metrics = calculate_general_metrics(baseline_predictions['id_labels_h1'], \n",
        "                                           baseline_predictions['id_logits_h1'], \n",
        "                                           COARSE_CLASSES, \"coarse\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fine classification (Head 2)\n",
        "valid_mask = baseline_predictions['id_labels_h2'] >= 0\n",
        "fine_metrics = calculate_general_metrics(baseline_predictions['id_labels_h2'][valid_mask], \n",
        "                                         baseline_predictions['id_logits_h2'][valid_mask], \n",
        "                                         FINE_CLASSES, \"fine\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "hierarchical_metrics = calculate_hierarchical_metrics(baseline_predictions, COARSE_CLASSES, FINE_CLASSES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "ood_metrics = calculate_ood_metrics(baseline_predictions['id_logits_h1'], baseline_predictions['ood_logits_h1'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "COARSE CLASSIFICATION (Head 1):\n",
            "  Macro F1: 0.2786\n",
            "  Balanced Accuracy: 0.3333\n",
            "  MCC: 0.0000\n",
            "  AUPRC Macro: 0.3847\n",
            "  ECE: 0.1380\n",
            "\n",
            "FINE CLASSIFICATION (Head 2):\n",
            "  Macro F1: 0.0929\n",
            "  Balanced Accuracy: 0.1583\n",
            "  MCC: 0.2202\n",
            "  AUPRC Macro: 0.1767\n",
            "  ECE: 0.1073\n",
            "\n",
            "HIERARCHICAL METRICS:\n",
            "  Exact Match: 0.2030\n",
            "  Coarse Correct: 0.3333\n",
            "  Fine Conditional F1: 0.0743\n",
            "\n",
            "OOD DETECTION:\n",
            "  AUROC: 0.4657\n",
            "  AUPRC: 0.7857\n",
            "  FPR@95%TPR: 0.7951\n",
            "  Detection Error: 0.7951\n",
            "\n",
            "PER-CLASS ANALYSIS - COARSE:\n",
            "\n",
            "PER-CLASS ANALYSIS - COARSE:\n",
            "           precision  recall    f1  auroc  auprc  support\n",
            "benign         0.718   1.000 0.836  0.597  0.780     7439\n",
            "malignant      0.000   0.000 0.000  0.620  0.344     2658\n",
            "no_lesion      0.000   0.000 0.000  0.488  0.031      262\n",
            "\n",
            "PER-CLASS ANALYSIS - FINE:\n",
            "           precision  recall    f1  auroc  auprc  support\n",
            "nv             0.817   0.501 0.621  0.818  0.780     3070\n",
            "mel            0.000   0.000 0.000  0.496  0.138      930\n",
            "bkl            0.117   0.920 0.208  0.702  0.204      615\n",
            "bcc            0.500   0.004 0.008  0.643  0.311     1284\n",
            "scc_akiec      0.000   0.000 0.000  0.499  0.070      441\n",
            "vasc           0.000   0.000 0.000  0.501  0.010       57\n",
            "df             0.000   0.000 0.000  0.707  0.019       66\n",
            "other          0.000   0.000 0.000  0.358  0.003       28\n",
            "no_lesion      0.000   0.000 0.000  0.661  0.055      262\n"
          ]
        }
      ],
      "source": [
        "all_results['baseline'] = {\n",
        "                'model_name': 'baseline',\n",
        "                'coarse_metrics': coarse_metrics,\n",
        "                'fine_metrics': fine_metrics,\n",
        "                'hierarchical_metrics': hierarchical_metrics,\n",
        "                'ood_metrics': ood_metrics\n",
        "            }\n",
        "\n",
        "print_metrics(coarse_metrics, fine_metrics, hierarchical_metrics, ood_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SSL model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "baseline_predictions = all_predictions['ssl']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Coarse classification (Head 1)\n",
        "coarse_metrics = calculate_general_metrics(baseline_predictions['id_labels_h1'], \n",
        "                                           baseline_predictions['id_logits_h1'], \n",
        "                                           COARSE_CLASSES, \"coarse\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fine classification (Head 2)\n",
        "valid_mask = baseline_predictions['id_labels_h2'] >= 0\n",
        "fine_metrics = calculate_general_metrics(baseline_predictions['id_labels_h2'][valid_mask], \n",
        "                                         baseline_predictions['id_logits_h2'][valid_mask], \n",
        "                                         FINE_CLASSES, \"fine\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hierarchical_metrics = calculate_hierarchical_metrics(baseline_predictions, COARSE_CLASSES, FINE_CLASSES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ood_metrics = calculate_ood_metrics(baseline_predictions['id_logits_h1'], baseline_predictions['ood_logits_h1'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_results['ssl'] = {\n",
        "                'model_name': 'ssl',\n",
        "                'coarse_metrics': coarse_metrics,\n",
        "                'fine_metrics': fine_metrics,\n",
        "                'hierarchical_metrics': hierarchical_metrics,\n",
        "                'ood_metrics': ood_metrics\n",
        "            }\n",
        "\n",
        "print_metrics(coarse_metrics, fine_metrics, hierarchical_metrics, ood_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ensemble model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Voting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ensemble_voting_predictions = all_predictions['ensemble_voting']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Coarse classification (Head 1)\n",
        "coarse_metrics = calculate_general_metrics(ensemble_voting_predictions['id_labels_h1'], \n",
        "                                           ensemble_voting_predictions['id_logits_h1'], \n",
        "                                           COARSE_CLASSES, \"coarse\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fine classification (Head 2)\n",
        "valid_mask = ensemble_voting_predictions['id_labels_h2'] >= 0\n",
        "fine_metrics = calculate_general_metrics(ensemble_voting_predictions['id_labels_h2'][valid_mask], \n",
        "                                         ensemble_voting_predictions['id_logits_h2'][valid_mask], \n",
        "                                         FINE_CLASSES, \"fine\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hierarchical_metrics = calculate_hierarchical_metrics(ensemble_voting_predictions, COARSE_CLASSES, FINE_CLASSES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ood_metrics = calculate_ood_metrics(ensemble_voting_predictions['id_logits_h1'], ensemble_voting_predictions['ood_logits_h1'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_results['ensemble_voting'] = {\n",
        "                'model_name': 'ensemble_voting',\n",
        "                'coarse_metrics': coarse_metrics,\n",
        "                'fine_metrics': fine_metrics,\n",
        "                'hierarchical_metrics': hierarchical_metrics,\n",
        "                'ood_metrics': ood_metrics\n",
        "            }\n",
        "\n",
        "print_metrics(coarse_metrics, fine_metrics, hierarchical_metrics, ood_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Weighting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ensemble_weighted_predictions = all_predictions['ensemble_weighted']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Coarse classification (Head 1)\n",
        "coarse_metrics = calculate_general_metrics(ensemble_weighted_predictions['id_labels_h1'], \n",
        "                                           ensemble_weighted_predictions['id_logits_h1'], \n",
        "                                           COARSE_CLASSES, \"coarse\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fine classification (Head 2)\n",
        "valid_mask = ensemble_weighted_predictions['id_labels_h2'] >= 0\n",
        "fine_metrics = calculate_general_metrics(ensemble_weighted_predictions['id_labels_h2'][valid_mask], \n",
        "                                         ensemble_weighted_predictions['id_logits_h2'][valid_mask], \n",
        "                                         FINE_CLASSES, \"fine\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hierarchical_metrics = calculate_hierarchical_metrics(ensemble_weighted_predictions, COARSE_CLASSES, FINE_CLASSES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ood_metrics = calculate_ood_metrics(ensemble_weighted_predictions['id_logits_h1'], ensemble_weighted_predictions['ood_logits_h1'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_results['ensemble_weighted'] = {\n",
        "                'model_name': 'ensemble_weighted',\n",
        "                'coarse_metrics': coarse_metrics,\n",
        "                'fine_metrics': fine_metrics,\n",
        "                'hierarchical_metrics': hierarchical_metrics,\n",
        "                'ood_metrics': ood_metrics\n",
        "            }\n",
        "\n",
        "print_metrics(coarse_metrics, fine_metrics, hierarchical_metrics, ood_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Individual ResNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resnet_predictions = all_predictions['resnet']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Coarse classification (Head 1)\n",
        "coarse_metrics = calculate_general_metrics(resnet_predictions['id_labels_h1'], \n",
        "                                           resnet_predictions['id_logits_h1'], \n",
        "                                           COARSE_CLASSES, \"coarse\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fine classification (Head 2)\n",
        "valid_mask = resnet_predictions['id_labels_h2'] >= 0\n",
        "fine_metrics = calculate_general_metrics(resnet_predictions['id_labels_h2'][valid_mask], \n",
        "                                         resnet_predictions['id_logits_h2'][valid_mask], \n",
        "                                         FINE_CLASSES, \"fine\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hierarchical_metrics = calculate_hierarchical_metrics(resnet_predictions, COARSE_CLASSES, FINE_CLASSES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ood_metrics = calculate_ood_metrics(resnet_predictions['id_logits_h1'], resnet_predictions['ood_logits_h1'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# all_results['resnet'] = {\n",
        "#                 'model_name': 'resnet',\n",
        "#                 'coarse_metrics': coarse_metrics,\n",
        "#                 'fine_metrics': fine_metrics,\n",
        "#                 'hierarchical_metrics': hierarchical_metrics,\n",
        "#                 'ood_metrics': ood_metrics\n",
        "#             }\n",
        "\n",
        "print_metrics(coarse_metrics, fine_metrics, hierarchical_metrics, ood_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Individual DenseNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "densenet_predictions = all_predictions['densenet']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Coarse classification (Head 1)\n",
        "coarse_metrics = calculate_general_metrics(densenet_predictions['id_labels_h1'], \n",
        "                                           densenet_predictions['id_logits_h1'], \n",
        "                                           COARSE_CLASSES, \"coarse\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fine classification (Head 2)\n",
        "valid_mask = densenet_predictions['id_labels_h2'] >= 0\n",
        "fine_metrics = calculate_general_metrics(densenet_predictions['id_labels_h2'][valid_mask], \n",
        "                                         densenet_predictions['id_logits_h2'][valid_mask], \n",
        "                                         FINE_CLASSES, \"fine\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hierarchical_metrics = calculate_hierarchical_metrics(densenet_predictions, COARSE_CLASSES, FINE_CLASSES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ood_metrics = calculate_ood_metrics(densenet_predictions['id_logits_h1'], densenet_predictions['ood_logits_h1'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# all_results['densenet'] = {\n",
        "#                 'model_name': 'densenet',\n",
        "#                 'coarse_metrics': coarse_metrics,\n",
        "#                 'fine_metrics': fine_metrics,\n",
        "#                 'hierarchical_metrics': hierarchical_metrics,\n",
        "#                 'ood_metrics': ood_metrics\n",
        "#             }\n",
        "\n",
        "print_metrics(coarse_metrics, fine_metrics, hierarchical_metrics, ood_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "comparison_data = []\n",
        "for model_key, result in all_results.items():\n",
        "    model_name = result['model_name']\n",
        "    comparison_data.append({\n",
        "        'Model': model_name,\n",
        "        # General metrics - Coarse\n",
        "        'Coarse Macro-F1': result['coarse_metrics']['macro_f1'],\n",
        "        'Coarse Balanced Acc': result['coarse_metrics']['balanced_accuracy'],\n",
        "        'Coarse MCC': result['coarse_metrics']['mcc'],\n",
        "        'Coarse AUPRC': result['coarse_metrics']['auprc_macro'],\n",
        "        'Coarse ECE': result['coarse_metrics']['ece'],\n",
        "        # General metrics - Fine\n",
        "        'Fine Macro-F1': result['fine_metrics']['macro_f1'],\n",
        "        'Fine Balanced Acc': result['fine_metrics']['balanced_accuracy'],\n",
        "        'Fine MCC': result['fine_metrics']['mcc'],\n",
        "        'Fine AUPRC': result['fine_metrics']['auprc_macro'],\n",
        "        'Fine ECE': result['fine_metrics']['ece'],\n",
        "        # Hierarchical metrics\n",
        "        'Exact Match': result['hierarchical_metrics']['exact_match'],\n",
        "        'Coarse Correct': result['hierarchical_metrics']['coarse_correct'],\n",
        "        'Fine Conditional F1': result['hierarchical_metrics']['fine_conditional_f1'],\n",
        "        # OOD metrics\n",
        "        'OOD AUROC': result['ood_metrics']['auroc'],\n",
        "        'OOD AUPRC': result['ood_metrics']['auprc'],\n",
        "        'FPR@95%TPR': result['ood_metrics']['fpr_at_95_tpr'],\n",
        "        'Detection Error': result['ood_metrics']['detection_error']\n",
        "    })\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(comparison_df.round(4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# General metrics\n",
        "best_coarse_f1 = comparison_df.loc[comparison_df['Coarse Macro-F1'].idxmax()]\n",
        "best_fine_f1 = comparison_df.loc[comparison_df['Fine Macro-F1'].idxmax()]\n",
        "best_coarse_ece = comparison_df.loc[comparison_df['Coarse ECE'].idxmin()]\n",
        "best_fine_ece = comparison_df.loc[comparison_df['Fine ECE'].idxmin()]\n",
        "\n",
        "# Hierarchical metrics\n",
        "best_exact_match = comparison_df.loc[comparison_df['Exact Match'].idxmax()]\n",
        "best_coarse_correct = comparison_df.loc[comparison_df['Coarse Correct'].idxmax()]\n",
        "best_fine_conditional = comparison_df.loc[comparison_df['Fine Conditional F1'].idxmax()]\n",
        "\n",
        "# OOD metrics\n",
        "best_ood_auroc = comparison_df.loc[comparison_df['OOD AUROC'].idxmax()]\n",
        "best_ood_auprc = comparison_df.loc[comparison_df['OOD AUPRC'].idxmax()]\n",
        "best_fpr_95 = comparison_df.loc[comparison_df['FPR@95%TPR'].idxmin()]\n",
        "best_detection_error = comparison_df.loc[comparison_df['Detection Error'].idxmin()]\n",
        "\n",
        "print(f\"📊 GENERAL EVALUATION:\")\n",
        "print(f\"  Best Coarse Macro-F1: {best_coarse_f1['Model']} ({best_coarse_f1['Coarse Macro-F1']:.4f})\")\n",
        "print(f\"  Best Fine Macro-F1: {best_fine_f1['Model']} ({best_fine_f1['Fine Macro-F1']:.4f})\")\n",
        "print(f\"  Best Coarse Calibration: {best_coarse_ece['Model']} (ECE: {best_coarse_ece['Coarse ECE']:.4f})\")\n",
        "print(f\"  Best Fine Calibration: {best_fine_ece['Model']} (ECE: {best_fine_ece['Fine ECE']:.4f})\")\n",
        "\n",
        "print(f\"\\n📊 HIERARCHICAL EVALUATION:\")\n",
        "print(f\"  Best Exact Match: {best_exact_match['Model']} ({best_exact_match['Exact Match']:.4f})\")\n",
        "print(f\"  Best Coarse Correct: {best_coarse_correct['Model']} ({best_coarse_correct['Coarse Correct']:.4f})\")\n",
        "print(f\"  Best Fine Conditional: {best_fine_conditional['Model']} ({best_fine_conditional['Fine Conditional F1']:.4f})\")\n",
        "\n",
        "print(f\"\\n📊 OOD DETECTION:\")\n",
        "print(f\"  Best OOD AUROC: {best_ood_auroc['Model']} ({best_ood_auroc['OOD AUROC']:.4f})\")\n",
        "print(f\"  Best OOD AUPRC: {best_ood_auprc['Model']} ({best_ood_auprc['OOD AUPRC']:.4f})\")\n",
        "print(f\"  Best FPR@95%TPR: {best_fpr_95['Model']} ({best_fpr_95['FPR@95%TPR']:.4f})\")\n",
        "print(f\"  Best Detection Error: {best_detection_error['Model']} ({best_detection_error['Detection Error']:.4f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "comparison_df['Overall Score'] = (\n",
        "    0.4 * comparison_df['Coarse Macro-F1'] +\n",
        "    0.3 * comparison_df['Fine Macro-F1'] +\n",
        "    0.2 * comparison_df['OOD AUROC'] +\n",
        "    0.1 * (1 - comparison_df['Fine ECE'])  # Lower ECE is better\n",
        ")\n",
        "\n",
        "# Rank models by overall performance\n",
        "ranked_models = comparison_df.sort_values('Overall Score', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "f, axes = plt.subplots(2, 3, figsize=(10, 12))\n",
        "f.suptitle('General Evaluation Metrics - Focused Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "models = comparison_df['Model'].tolist()\n",
        "colors = [MODELS_CONFIG.get(key, {}).get('color', '#666666') for key in all_results.keys()]\n",
        "\n",
        "# Coarse metrics\n",
        "axes[0, 0].bar(models, comparison_df['Coarse Macro-F1'], color=colors, alpha=0.7)\n",
        "axes[0, 0].set_title('Coarse Macro-F1 Score (Primary Metric)', fontweight='bold')\n",
        "axes[0, 0].set_ylabel('Macro-F1')\n",
        "axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[0, 1].bar(models, comparison_df['Coarse Balanced Acc'], color=colors, alpha=0.7)\n",
        "axes[0, 1].set_title('Coarse Balanced Accuracy', fontweight='bold')\n",
        "axes[0, 1].set_ylabel('Balanced Accuracy')\n",
        "axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[0, 2].bar(models, comparison_df['Coarse MCC'], color=colors, alpha=0.7)\n",
        "axes[0, 2].set_title('Coarse Matthews Correlation Coefficient', fontweight='bold')\n",
        "axes[0, 2].set_ylabel('MCC')\n",
        "axes[0, 2].tick_params(axis='x', rotation=45)\n",
        "axes[0, 2].grid(True, alpha=0.3)\n",
        "\n",
        "# Fine metrics\n",
        "axes[1, 0].bar(models, comparison_df['Fine Macro-F1'], color=colors, alpha=0.7)\n",
        "axes[1, 0].set_title('Fine Macro-F1 Score', fontweight='bold')\n",
        "axes[1, 0].set_ylabel('Macro-F1')\n",
        "axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1, 1].bar(models, comparison_df['Fine AUPRC'], color=colors, alpha=0.7)\n",
        "axes[1, 1].set_title('Fine AUPRC Macro', fontweight='bold')\n",
        "axes[1, 1].set_ylabel('AUPRC')\n",
        "axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1, 2].bar(models, comparison_df['Fine ECE'], color=colors, alpha=0.7)\n",
        "axes[1, 2].set_title('Fine Expected Calibration Error', fontweight='bold')\n",
        "axes[1, 2].set_ylabel('ECE (Lower is Better)')\n",
        "axes[1, 2].tick_params(axis='x', rotation=45)\n",
        "axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "f, axes = plt.subplots(1, 3, figsize=(10, 6))\n",
        "f.suptitle('Hierarchical Evaluation Metrics', fontsize=16, fontweight='bold')\n",
        "\n",
        "axes[0].bar(models, comparison_df['Exact Match'], color=colors, alpha=0.7)\n",
        "axes[0].set_title('Exact Match (Both Heads Correct)', fontweight='bold')\n",
        "axes[0].set_ylabel('Exact Match Rate')\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].bar(models, comparison_df['Coarse Correct'], color=colors, alpha=0.7)\n",
        "axes[1].set_title('Coarse Classification Accuracy', fontweight='bold')\n",
        "axes[1].set_ylabel('Balanced Accuracy')\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[2].bar(models, comparison_df['Fine Conditional F1'], color=colors, alpha=0.7)\n",
        "axes[2].set_title('Fine Classification F1 (Conditional)', fontweight='bold')\n",
        "axes[2].set_ylabel('Macro-F1')\n",
        "axes[2].tick_params(axis='x', rotation=45)\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "f, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
        "f.suptitle('Out-of-Distribution Detection Performance', fontsize=16, fontweight='bold')\n",
        "\n",
        "axes[0, 0].bar(models, comparison_df['OOD AUROC'], color=colors, alpha=0.7)\n",
        "axes[0, 0].set_title('OOD Detection AUROC', fontweight='bold')\n",
        "axes[0, 0].set_ylabel('AUROC')\n",
        "axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[0, 1].bar(models, comparison_df['OOD AUPRC'], color=colors, alpha=0.7)\n",
        "axes[0, 1].set_title('OOD Detection AUPRC', fontweight='bold')\n",
        "axes[0, 1].set_ylabel('AUPRC')\n",
        "axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1, 0].bar(models, comparison_df['FPR@95%TPR'], color=colors, alpha=0.7)\n",
        "axes[1, 0].set_title('FPR@95%TPR (Lower is Better)', fontweight='bold')\n",
        "axes[1, 0].set_ylabel('FPR')\n",
        "axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1, 1].bar(models, comparison_df['Detection Error'], color=colors, alpha=0.7)\n",
        "axes[1, 1].set_title('Detection Error (Lower is Better)', fontweight='bold')\n",
        "axes[1, 1].set_ylabel('Detection Error')\n",
        "axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# FINAL SUMMARY AND RECOMMENDATIONS\n",
        "# =============================================================================\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"FINAL EVALUATION SUMMARY\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "# Calculate overall performance score (weighted combination of key metrics)\n",
        "# Primary metrics: Coarse Macro-F1 (40%), Fine Macro-F1 (30%), OOD AUROC (20%), ECE (10%)\n",
        "comparison_df['Overall Score'] = (\n",
        "    0.4 * comparison_df['Coarse Macro-F1'] +\n",
        "    0.3 * comparison_df['Fine Macro-F1'] +\n",
        "    0.2 * comparison_df['OOD AUROC'] +\n",
        "    0.1 * (1 - comparison_df['Fine ECE'])  # Lower ECE is better\n",
        ")\n",
        "\n",
        "# Rank models by overall performance\n",
        "ranked_models = comparison_df.sort_values('Overall Score', ascending=False)\n",
        "\n",
        "print(f\"\\n📊 OVERALL MODEL RANKING:\")\n",
        "print(f\"{'-'*50}\")\n",
        "for i, (_, row) in enumerate(ranked_models.iterrows(), 1):\n",
        "    print(f\"{i}. {row['Model']}: {row['Overall Score']:.4f}\")\n",
        "    print(f\"   Coarse F1: {row['Coarse Macro-F1']:.4f}, Fine F1: {row['Fine Macro-F1']:.4f}\")\n",
        "    print(f\"   OOD AUROC: {row['OOD AUROC']:.4f}, ECE: {row['Fine ECE']:.4f}\")\n",
        "\n",
        "# Key insights\n",
        "print(f\"\\n🎯 KEY INSIGHTS:\")\n",
        "print(f\"{'-'*20}\")\n",
        "\n",
        "best_model = ranked_models.iloc[0]\n",
        "baseline_model = ranked_models[ranked_models['Model'].str.contains('Baseline')]\n",
        "\n",
        "if len(baseline_model) > 0:\n",
        "    baseline_score = baseline_model.iloc[0]['Overall Score']\n",
        "    improvement = ((best_model['Overall Score'] - baseline_score) / baseline_score) * 100\n",
        "    print(f\"• Best model improves over baseline by {improvement:.2f}%\")\n",
        "\n",
        "# Check ensemble performance\n",
        "ensemble_models = ranked_models[ranked_models['Model'].str.contains('Ensemble')]\n",
        "if len(ensemble_models) > 0:\n",
        "    print(f\"• Ensemble methods show superior performance\")\n",
        "    print(f\"  - Best ensemble: {ensemble_models.iloc[0]['Model']}\")\n",
        "\n",
        "# Check SSL performance\n",
        "ssl_models = ranked_models[ranked_models['Model'].str.contains('SSL')]\n",
        "if len(ssl_models) > 0:\n",
        "    print(f\"• SSL fine-tuning demonstrates clear benefits\")\n",
        "    print(f\"  - SSL model rank: {ranked_models[ranked_models['Model'].str.contains('SSL')].index[0] + 1}\")\n",
        "\n",
        "# Clinical recommendations\n",
        "print(f\"\\n💡 CLINICAL RECOMMENDATIONS:\")\n",
        "print(f\"{'-'*30}\")\n",
        "\n",
        "# Best model for different use cases\n",
        "best_coarse = ranked_models.iloc[ranked_models['Coarse Macro-F1'].idxmax()]\n",
        "best_fine = ranked_models.iloc[ranked_models['Fine Macro-F1'].idxmax()]\n",
        "best_ood = ranked_models.iloc[ranked_models['OOD AUROC'].idxmax()]\n",
        "best_calibrated = ranked_models.iloc[ranked_models['Fine ECE'].idxmin()]\n",
        "\n",
        "print(f\"1. **Primary Classification**: {best_coarse['Model']} (Coarse F1: {best_coarse['Coarse Macro-F1']:.4f})\")\n",
        "print(f\"2. **Fine-grained Diagnosis**: {best_fine['Model']} (Fine F1: {best_fine['Fine Macro-F1']:.4f})\")\n",
        "print(f\"3. **Uncertainty Detection**: {best_ood['Model']} (OOD AUROC: {best_ood['OOD AUROC']:.4f})\")\n",
        "print(f\"4. **Confidence Calibration**: {best_calibrated['Model']} (ECE: {best_calibrated['Fine ECE']:.4f})\")\n",
        "\n",
        "# Production recommendations\n",
        "print(f\"\\n🚀 PRODUCTION DEPLOYMENT:\")\n",
        "print(f\"{'-'*25}\")\n",
        "print(f\"• **Recommended Model**: {best_model['Model']}\")\n",
        "print(f\"• **Confidence Threshold**: Use detection error optimal threshold\")\n",
        "print(f\"• **Monitoring**: Track both coarse and fine performance\")\n",
        "print(f\"• **Fallback**: Human expert review for OOD cases\")\n",
        "\n",
        "# Performance thresholds\n",
        "print(f\"\\n📈 PERFORMANCE THRESHOLDS:\")\n",
        "print(f\"{'-'*25}\")\n",
        "print(f\"• Coarse Macro-F1 ≥ 0.80: {'✓' if best_model['Coarse Macro-F1'] >= 0.80 else '✗'}\")\n",
        "print(f\"• Fine Macro-F1 ≥ 0.70: {'✓' if best_model['Fine Macro-F1'] >= 0.70 else '✗'}\")\n",
        "print(f\"• OOD AUROC ≥ 0.80: {'✓' if best_model['OOD AUROC'] >= 0.80 else '✗'}\")\n",
        "print(f\"• ECE ≤ 0.10: {'✓' if best_model['Fine ECE'] <= 0.10 else '✗'}\")\n",
        "\n",
        "print(f\"\\n✅ EVALUATION COMPLETED SUCCESSFULLY!\")\n",
        "print(f\"📁 Results saved and ready for analysis\")\n",
        "print(f\"🎯 Best performing model: {best_model['Model']}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tfm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
