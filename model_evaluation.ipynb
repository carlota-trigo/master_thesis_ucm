{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f751837b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f6d823",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 999\n",
    "DATA_DIR = Path(\"../data\")\n",
    "PREPARED_CSV = DATA_DIR / \"training_prepared_data.csv\"\n",
    "IMAGE_PATH = DATA_DIR.joinpath(\"images\", \"images\")\n",
    "\n",
    "# Model paths\n",
    "BASELINE_DIR = Path(\"./outputs/simple_twohead_b0_v2\")\n",
    "SSL_DIR = Path(\"./outputs/ssl_finetuned\")\n",
    "ENSEMBLE_DIR = Path(\"./outputs/ensemble_models\")\n",
    "INDIVIDUAL_DIR = Path(\"./outputs/individual_models\")\n",
    "\n",
    "# Model configurations\n",
    "MODELS_CONFIG = {\n",
    "    'baseline': {\n",
    "        'name': 'Baseline EfficientNetB1',\n",
    "        'path': BASELINE_DIR / \"best_model.keras\",\n",
    "        'type': 'single',\n",
    "        'color': '#1f77b4'\n",
    "    },\n",
    "    'ssl': {\n",
    "        'name': 'SSL Fine-tuned',\n",
    "        'path': SSL_DIR / \"finetuned_best_model.keras\", \n",
    "        'type': 'single',\n",
    "        'color': '#ff7f0e'\n",
    "    },\n",
    "    'ensemble_voting': {\n",
    "        'name': 'Voting Ensemble',\n",
    "        'path': ENSEMBLE_DIR,\n",
    "        'type': 'ensemble',\n",
    "        'color': '#2ca02c'\n",
    "    },\n",
    "    'ensemble_weighted': {\n",
    "        'name': 'Weighted Ensemble',\n",
    "        'path': ENSEMBLE_DIR,\n",
    "        'type': 'ensemble', \n",
    "        'color': '#d62728'\n",
    "    }\n",
    "}\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# --- Class Definitions (MUST match training) ---\n",
    "DX_CLASSES = sorted(['nv', 'mel', 'bkl', 'bcc', 'scc_akiec', 'vasc', 'df', 'other', 'no_lesion'])\n",
    "LESION_TYPE_CLASSES = [\"benign\", \"malignant\", \"no_lesion\"]\n",
    "N_DX_CLASSES = len(DX_CLASSES)\n",
    "N_LESION_TYPE_CLASSES = len(LESION_TYPE_CLASSES)\n",
    "\n",
    "print(\"Model Evaluation Configuration:\")\n",
    "print(\"=\" * 50)\n",
    "for model_key, config in MODELS_CONFIG.items():\n",
    "    print(f\"{model_key}: {config['name']}\")\n",
    "    print(f\"  Path: {config['path']}\")\n",
    "    print(f\"  Type: {config['type']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ddf15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_augmenter(is_training):\n",
    "    if is_training:\n",
    "        raise ValueError(\"build_augmenter should not be called with is_training=True during evaluation.\")\n",
    "    return keras.Sequential([\n",
    "        layers.Resizing(256, 256),\n",
    "        layers.CenterCrop(IMG_SIZE, IMG_SIZE),\n",
    "    ], name=\"preprocessor\")\n",
    "\n",
    "def build_dataset(df, is_training=False):\n",
    "    if is_training:\n",
    "        raise ValueError(\"build_dataset should not be called with is_training=True during evaluation.\")\n",
    "\n",
    "    df = df.dropna(subset=['image_path', 'head2_idx']).copy()\n",
    "    df_fine = df['head1_idx'].fillna(-1).astype('int32').values\n",
    "    df_coarse = df['head2_idx'].astype('int32').values\n",
    "\n",
    "    def resolve_path(p):\n",
    "        p = str(p)\n",
    "        return p if os.path.isabs(p) else str(IMAGE_PATH / p)\n",
    "\n",
    "    img_paths = df['image_path'].astype(str).apply(resolve_path).tolist()\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices((img_paths, df_fine, df_coarse))\n",
    "\n",
    "    augmenter = build_augmenter(is_training)\n",
    "    rescale = layers.Rescaling(1./255)\n",
    "    normalization_layer = layers.Normalization(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        variance=[0.229**2, 0.224**2, 0.225**2]\n",
    "    )\n",
    "\n",
    "    def load_and_preprocess(path, label_fine, label_coarse):\n",
    "        img = tf.io.read_file(path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = augmenter(img)\n",
    "        img = rescale(img)\n",
    "        img = normalization_layer(img)\n",
    "        return img, {\"fine_output\": label_fine, \"coarse_output\": label_coarse}\n",
    "\n",
    "    ds = ds.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "def masked_sparse_categorical_crossentropy(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.int32)\n",
    "    mask = tf.cast(tf.not_equal(y_true, -1), dtype=tf.float32)\n",
    "    loss = keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
    "    masked_loss = loss * mask\n",
    "    return tf.reduce_sum(masked_loss) / (tf.reduce_sum(mask) + 1e-8)\n",
    "\n",
    "def create_two_head_model(n_fine, n_coarse, img_size=IMG_SIZE, dropout=0.2):\n",
    "    \"\"\"Creates the two-headed model using the Keras Functional API.\"\"\"\n",
    "    inputs = keras.Input(shape=(img_size, img_size, 3), name=\"input\")\n",
    "    \n",
    "    backbone = keras.applications.EfficientNetB1(\n",
    "        include_top=False, \n",
    "        weights=\"imagenet\", \n",
    "        input_tensor=inputs\n",
    "    )\n",
    "    \n",
    "    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(backbone.output)\n",
    "    x = layers.Dropout(dropout, name=\"top_dropout\")(x)\n",
    "\n",
    "    output_fine = layers.Dense(n_fine, name=\"fine_output\")(x)\n",
    "    output_coarse = layers.Dense(n_coarse, name=\"coarse_output\")(x)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=[output_fine, output_coarse], name=\"EffB1TwoHead\")\n",
    "    return model\n",
    "\n",
    "def load_individual_model(model_path):\n",
    "    \"\"\"Load individual model from path.\"\"\"\n",
    "    try:\n",
    "        model = create_two_head_model(N_DX_CLASSES, N_LESION_TYPE_CLASSES)\n",
    "        model.load_weights(str(model_path))\n",
    "        print(f\"âœ“ Loaded model from {model_path}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Failed to load model from {model_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_ensemble_models():\n",
    "    \"\"\"Load individual models for ensemble.\"\"\"\n",
    "    ensemble_models = {}\n",
    "    backbone_types = ['efficientnet', 'resnet', 'densenet']\n",
    "    \n",
    "    for backbone_type in backbone_types:\n",
    "        model_path = INDIVIDUAL_DIR / backbone_type / f\"{backbone_type}_best_model.keras\"\n",
    "        model = load_individual_model(model_path)\n",
    "        if model is not None:\n",
    "            ensemble_models[backbone_type] = model\n",
    "    \n",
    "    return ensemble_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a036bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = 'outputs\\simple_twohead_b0\\best_model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m     21\u001b[0m model \u001b[38;5;241m=\u001b[39m create_two_head_model(N_DX_CLASSES, N_LESION_TYPE_CLASSES)\n\u001b[1;32m---> 22\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mOUTDIR\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbest_model.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\msi\\anaconda3\\envs\\tfm\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\msi\\anaconda3\\envs\\tfm\\Lib\\site-packages\\h5py\\_hl\\files.py:564\u001b[0m, in \u001b[0;36mFile.__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[0;32m    555\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[0;32m    556\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[0;32m    557\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[0;32m    558\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[0;32m    559\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[0;32m    560\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    561\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[0;32m    562\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[0;32m    563\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[1;32m--> 564\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    567\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[1;32mc:\\Users\\msi\\anaconda3\\envs\\tfm\\Lib\\site-packages\\h5py\\_hl\\files.py:238\u001b[0m, in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[0;32m    237\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[1;32m--> 238\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    240\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[1;32mh5py/_objects.pyx:56\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py/_objects.pyx:57\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py/h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = 'outputs\\simple_twohead_b0\\best_model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "# Load all models\n",
    "print(\"Loading models...\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "models = {}\n",
    "ensemble_models = {}\n",
    "\n",
    "# Load individual models\n",
    "for model_key, config in MODELS_CONFIG.items():\n",
    "    if config['type'] == 'single':\n",
    "        model = load_individual_model(config['path'])\n",
    "        if model is not None:\n",
    "            models[model_key] = model\n",
    "\n",
    "# Load ensemble models\n",
    "ensemble_models = load_ensemble_models()\n",
    "\n",
    "print(f\"\\nLoaded {len(models)} individual models\")\n",
    "print(f\"Loaded {len(ensemble_models)} ensemble component models\")\n",
    "\n",
    "# Create ensemble predictions\n",
    "def create_voting_ensemble(models_dict, dataset):\n",
    "    \"\"\"Create voting ensemble from multiple models.\"\"\"\n",
    "    all_fine_preds = []\n",
    "    all_coarse_preds = []\n",
    "    \n",
    "    for backbone_type, model in models_dict.items():\n",
    "        print(f\"Getting predictions from {backbone_type}...\")\n",
    "        preds = model.predict(dataset, verbose=0)\n",
    "        all_fine_preds.append(preds[0])\n",
    "        all_coarse_preds.append(preds[1])\n",
    "    \n",
    "    # Average predictions\n",
    "    ensemble_fine_preds = np.mean(all_fine_preds, axis=0)\n",
    "    ensemble_coarse_preds = np.mean(all_coarse_preds, axis=0)\n",
    "    \n",
    "    return ensemble_fine_preds, ensemble_coarse_preds\n",
    "\n",
    "def create_weighted_ensemble(models_dict, dataset, weights=None):\n",
    "    \"\"\"Create weighted ensemble from multiple models.\"\"\"\n",
    "    if weights is None:\n",
    "        weights = [1.0] * len(models_dict)\n",
    "    \n",
    "    weights = np.array(weights)\n",
    "    weights = weights / weights.sum()\n",
    "    \n",
    "    all_fine_preds = []\n",
    "    all_coarse_preds = []\n",
    "    \n",
    "    for i, (backbone_type, model) in enumerate(models_dict.items()):\n",
    "        print(f\"Getting predictions from {backbone_type} (weight: {weights[i]:.3f})...\")\n",
    "        preds = model.predict(dataset, verbose=0)\n",
    "        all_fine_preds.append(preds[0] * weights[i])\n",
    "        all_coarse_preds.append(preds[1] * weights[i])\n",
    "    \n",
    "    # Weighted average predictions\n",
    "    ensemble_fine_preds = np.sum(all_fine_preds, axis=0)\n",
    "    ensemble_coarse_preds = np.sum(all_coarse_preds, axis=0)\n",
    "    \n",
    "    return ensemble_fine_preds, ensemble_coarse_preds\n",
    "\n",
    "# Add ensemble models to models dict\n",
    "if len(ensemble_models) > 0:\n",
    "    print(\"\\nCreating ensemble predictions...\")\n",
    "    # This will be done later when we have the test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4379b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "df = pd.read_csv(PREPARED_CSV)\n",
    "test_df = df[df.split == \"test\"].copy()\n",
    "ood_df = df[df.split == \"test_ood\"].copy()\n",
    "\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "print(f\"OOD samples: {len(ood_df)}\")\n",
    "\n",
    "test_ds = build_dataset(test_df)\n",
    "ood_ds = build_dataset(ood_df)\n",
    "\n",
    "def get_predictions_and_labels(model, dataset):\n",
    "    \"\"\"Get predictions and labels from a model.\"\"\"\n",
    "    all_labels_h1, all_labels_h2 = [], []\n",
    "    all_logits_h1, all_logits_h2 = [], []\n",
    "\n",
    "    for images, labels in dataset:\n",
    "        logits_h1, logits_h2 = model.predict_on_batch(images)\n",
    "        all_logits_h1.append(logits_h1)\n",
    "        all_logits_h2.append(logits_h2)\n",
    "\n",
    "        all_labels_h1.append(labels['fine_output'].numpy())\n",
    "        all_labels_h2.append(labels['coarse_output'].numpy())\n",
    "        \n",
    "    all_logits_h1 = np.concatenate(all_logits_h1, axis=0)\n",
    "    all_logits_h2 = np.concatenate(all_logits_h2, axis=0)\n",
    "    all_labels_h1 = np.concatenate(all_labels_h1, axis=0)\n",
    "    all_labels_h2 = np.concatenate(all_labels_h2, axis=0)\n",
    "\n",
    "    return all_labels_h1, all_logits_h1, all_labels_h2, all_logits_h2\n",
    "\n",
    "def get_ensemble_predictions(ensemble_models, dataset, method='voting'):\n",
    "    \"\"\"Get ensemble predictions.\"\"\"\n",
    "    if method == 'voting':\n",
    "        fine_preds, coarse_preds = create_voting_ensemble(ensemble_models, dataset)\n",
    "    elif method == 'weighted':\n",
    "        fine_preds, coarse_preds = create_weighted_ensemble(ensemble_models, dataset)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'voting' or 'weighted'\")\n",
    "    \n",
    "    # Get labels from first model (all should be the same)\n",
    "    first_model = list(ensemble_models.values())[0]\n",
    "    labels_h1, _, labels_h2, _ = get_predictions_and_labels(first_model, dataset)\n",
    "    \n",
    "    return labels_h1, fine_preds, labels_h2, coarse_preds\n",
    "\n",
    "# Get predictions for all models\n",
    "print(\"\\nGetting predictions for all models...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "all_predictions = {}\n",
    "\n",
    "# Individual models\n",
    "for model_key, model in models.items():\n",
    "    print(f\"\\nEvaluating {MODELS_CONFIG[model_key]['name']}...\")\n",
    "    id_labels_h1, id_logits_h1, id_labels_h2, id_logits_h2 = get_predictions_and_labels(model, test_ds)\n",
    "    ood_labels_h1, ood_logits_h1, ood_labels_h2, ood_logits_h2 = get_predictions_and_labels(model, ood_ds)\n",
    "    \n",
    "    all_predictions[model_key] = {\n",
    "        'id_labels_h1': id_labels_h1, 'id_logits_h1': id_logits_h1,\n",
    "        'id_labels_h2': id_labels_h2, 'id_logits_h2': id_logits_h2,\n",
    "        'ood_labels_h1': ood_labels_h1, 'ood_logits_h1': ood_logits_h1,\n",
    "        'ood_labels_h2': ood_labels_h2, 'ood_logits_h2': ood_logits_h2\n",
    "    }\n",
    "\n",
    "# Ensemble models\n",
    "if len(ensemble_models) > 0:\n",
    "    print(f\"\\nEvaluating ensemble models...\")\n",
    "    \n",
    "    # Voting ensemble\n",
    "    id_labels_h1, id_logits_h1, id_labels_h2, id_logits_h2 = get_ensemble_predictions(ensemble_models, test_ds, 'voting')\n",
    "    ood_labels_h1, ood_logits_h1, ood_labels_h2, ood_logits_h2 = get_ensemble_predictions(ensemble_models, ood_ds, 'voting')\n",
    "    \n",
    "    all_predictions['ensemble_voting'] = {\n",
    "        'id_labels_h1': id_labels_h1, 'id_logits_h1': id_logits_h1,\n",
    "        'id_labels_h2': id_labels_h2, 'id_logits_h2': id_logits_h2,\n",
    "        'ood_labels_h1': ood_labels_h1, 'ood_logits_h1': ood_logits_h1,\n",
    "        'ood_labels_h2': ood_labels_h2, 'ood_logits_h2': ood_logits_h2\n",
    "    }\n",
    "    \n",
    "    # Weighted ensemble\n",
    "    id_labels_h1, id_logits_h1, id_labels_h2, id_logits_h2 = get_ensemble_predictions(ensemble_models, test_ds, 'weighted')\n",
    "    ood_labels_h1, ood_logits_h1, ood_labels_h2, ood_logits_h2 = get_ensemble_predictions(ensemble_models, ood_ds, 'weighted')\n",
    "    \n",
    "    all_predictions['ensemble_weighted'] = {\n",
    "        'id_labels_h1': id_labels_h1, 'id_logits_h1': id_logits_h1,\n",
    "        'id_labels_h2': id_labels_h2, 'id_logits_h2': id_logits_h2,\n",
    "        'ood_labels_h1': ood_labels_h1, 'ood_logits_h1': ood_logits_h1,\n",
    "        'ood_labels_h2': ood_labels_h2, 'ood_logits_h2': ood_logits_h2\n",
    "    }\n",
    "\n",
    "print(f\"\\nâœ“ Completed predictions for {len(all_predictions)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a991ae40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(labels, preds, class_names, title):\n",
    "    \"\"\"Plot confusion matrix.\"\"\"\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show()\n",
    "\n",
    "def calculate_metrics(labels, preds, class_names):\n",
    "    \"\"\"Calculate comprehensive metrics.\"\"\"\n",
    "    # Filter out masked samples (label == -1)\n",
    "    valid_mask = labels >= 0\n",
    "    valid_labels = labels[valid_mask]\n",
    "    valid_preds = preds[valid_mask]\n",
    "    \n",
    "    if len(valid_labels) == 0:\n",
    "        return {\n",
    "            'accuracy': 0.0,\n",
    "            'precision': 0.0,\n",
    "            'recall': 0.0,\n",
    "            'f1': 0.0,\n",
    "            'macro_f1': 0.0,\n",
    "            'weighted_f1': 0.0\n",
    "        }\n",
    "    \n",
    "    accuracy = accuracy_score(valid_labels, valid_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        valid_labels, valid_preds, average=None, zero_division=0\n",
    "    )\n",
    "    \n",
    "    macro_precision = np.mean(precision)\n",
    "    macro_recall = np.mean(recall)\n",
    "    macro_f1 = np.mean(f1)\n",
    "    \n",
    "    weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(\n",
    "        valid_labels, valid_preds, average='weighted', zero_division=0\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': macro_precision,\n",
    "        'recall': macro_recall,\n",
    "        'f1': macro_f1,\n",
    "        'weighted_f1': weighted_f1,\n",
    "        'per_class_f1': f1\n",
    "    }\n",
    "\n",
    "# Evaluate all models\n",
    "print(\"EVALUATING ALL MODELS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "all_metrics = {}\n",
    "\n",
    "for model_key, predictions in all_predictions.items():\n",
    "    model_name = MODELS_CONFIG[model_key]['name']\n",
    "    print(f\"\\n{'='*20} {model_name} {'='*20}\")\n",
    "    \n",
    "    # Fine-grained evaluation (Head 1)\n",
    "    id_preds_h1 = np.argmax(predictions['id_logits_h1'], axis=1)\n",
    "    valid_mask_h1 = predictions['id_labels_h1'] >= 0\n",
    "    valid_labels_h1 = predictions['id_labels_h1'][valid_mask_h1]\n",
    "    valid_preds_h1 = id_preds_h1[valid_mask_h1]\n",
    "    \n",
    "    print(f\"\\nFine-grained Classification Report:\")\n",
    "    if len(valid_labels_h1) > 0:\n",
    "        print(classification_report(valid_labels_h1, valid_preds_h1, target_names=DX_CLASSES))\n",
    "        fine_metrics = calculate_metrics(predictions['id_labels_h1'], id_preds_h1, DX_CLASSES)\n",
    "    else:\n",
    "        print(\"No valid fine-grained samples\")\n",
    "        fine_metrics = {'accuracy': 0.0, 'f1': 0.0, 'weighted_f1': 0.0}\n",
    "    \n",
    "    # Coarse evaluation (Head 2)\n",
    "    id_preds_h2 = np.argmax(predictions['id_logits_h2'], axis=1)\n",
    "    print(f\"\\nCoarse Classification Report:\")\n",
    "    print(classification_report(predictions['id_labels_h2'], id_preds_h2, target_names=LESION_TYPE_CLASSES))\n",
    "    coarse_metrics = calculate_metrics(predictions['id_labels_h2'], id_preds_h2, LESION_TYPE_CLASSES)\n",
    "    \n",
    "    # Store metrics\n",
    "    all_metrics[model_key] = {\n",
    "        'fine_accuracy': fine_metrics['accuracy'],\n",
    "        'fine_f1': fine_metrics['f1'],\n",
    "        'fine_weighted_f1': fine_metrics['weighted_f1'],\n",
    "        'coarse_accuracy': coarse_metrics['accuracy'],\n",
    "        'coarse_f1': coarse_metrics['f1'],\n",
    "        'coarse_weighted_f1': coarse_metrics['weighted_f1']\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nSummary Metrics:\")\n",
    "    print(f\"Fine-grained Accuracy: {fine_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Fine-grained F1: {fine_metrics['f1']:.4f}\")\n",
    "    print(f\"Coarse Accuracy: {coarse_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Coarse F1: {coarse_metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5417d76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPREHENSIVE MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "for model_key, metrics in all_metrics.items():\n",
    "    model_name = MODELS_CONFIG[model_key]['name']\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'Fine Accuracy': metrics['fine_accuracy'],\n",
    "        'Fine F1': metrics['fine_f1'],\n",
    "        'Fine Weighted F1': metrics['fine_weighted_f1'],\n",
    "        'Coarse Accuracy': metrics['coarse_accuracy'],\n",
    "        'Coarse F1': metrics['coarse_f1'],\n",
    "        'Coarse Weighted F1': metrics['coarse_weighted_f1']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nDetailed Comparison Table:\")\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# Find best models\n",
    "best_fine_acc = comparison_df.loc[comparison_df['Fine Accuracy'].idxmax()]\n",
    "best_fine_f1 = comparison_df.loc[comparison_df['Fine F1'].idxmax()]\n",
    "best_coarse_acc = comparison_df.loc[comparison_df['Coarse Accuracy'].idxmax()]\n",
    "best_coarse_f1 = comparison_df.loc[comparison_df['Coarse F1'].idxmax()]\n",
    "\n",
    "print(f\"\\nðŸ† BEST PERFORMING MODELS:\")\n",
    "print(f\"Best Fine-grained Accuracy: {best_fine_acc['Model']} ({best_fine_acc['Fine Accuracy']:.4f})\")\n",
    "print(f\"Best Fine-grained F1: {best_fine_f1['Model']} ({best_fine_f1['Fine F1']:.4f})\")\n",
    "print(f\"Best Coarse Accuracy: {best_coarse_acc['Model']} ({best_coarse_acc['Coarse Accuracy']:.4f})\")\n",
    "print(f\"Best Coarse F1: {best_coarse_f1['Model']} ({best_coarse_f1['Coarse F1']:.4f})\")\n",
    "\n",
    "# Calculate improvements\n",
    "baseline_metrics = all_metrics.get('baseline', {})\n",
    "if baseline_metrics:\n",
    "    print(f\"\\nðŸ“ˆ IMPROVEMENTS OVER BASELINE:\")\n",
    "    for model_key, metrics in all_metrics.items():\n",
    "        if model_key != 'baseline':\n",
    "            model_name = MODELS_CONFIG[model_key]['name']\n",
    "            fine_acc_improvement = (metrics['fine_accuracy'] - baseline_metrics['fine_accuracy']) / baseline_metrics['fine_accuracy'] * 100\n",
    "            fine_f1_improvement = (metrics['fine_f1'] - baseline_metrics['fine_f1']) / baseline_metrics['fine_f1'] * 100\n",
    "            coarse_acc_improvement = (metrics['coarse_accuracy'] - baseline_metrics['coarse_accuracy']) / baseline_metrics['coarse_accuracy'] * 100\n",
    "            coarse_f1_improvement = (metrics['coarse_f1'] - baseline_metrics['coarse_f1']) / baseline_metrics['coarse_f1'] * 100\n",
    "            \n",
    "            print(f\"\\n{model_name}:\")\n",
    "            print(f\"  Fine Accuracy: {fine_acc_improvement:+.2f}%\")\n",
    "            print(f\"  Fine F1: {fine_f1_improvement:+.2f}%\")\n",
    "            print(f\"  Coarse Accuracy: {coarse_acc_improvement:+.2f}%\")\n",
    "            print(f\"  Coarse F1: {coarse_f1_improvement:+.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfff444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 901ms/step - coarse_output_acc: 0.9360 - coarse_output_loss: 843.2475 - fine_output_acc: 0.6915 - fine_output_loss: 1626.9020 - loss: 2458.8330\n",
      "\n",
      "== Aggregate metrics ==\n",
      "coarse_output_acc: 0.9360\n",
      "coarse_output_loss: 843.2475\n",
      "fine_output_acc: 0.6915\n",
      "fine_output_loss: 1626.9020\n",
      "loss: 2458.8330\n"
     ]
    }
   ],
   "source": [
    "# Visualization of model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Fine-grained accuracy comparison\n",
    "models = comparison_df['Model'].tolist()\n",
    "fine_acc = comparison_df['Fine Accuracy'].tolist()\n",
    "fine_f1 = comparison_df['Fine F1'].tolist()\n",
    "coarse_acc = comparison_df['Coarse Accuracy'].tolist()\n",
    "coarse_f1 = comparison_df['Coarse F1'].tolist()\n",
    "\n",
    "colors = [MODELS_CONFIG.get(key, {}).get('color', '#666666') for key in all_metrics.keys()]\n",
    "\n",
    "axes[0, 0].bar(models, fine_acc, color=colors, alpha=0.7)\n",
    "axes[0, 0].set_title('Fine-grained Accuracy Comparison')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].bar(models, fine_f1, color=colors, alpha=0.7)\n",
    "axes[0, 1].set_title('Fine-grained F1 Score Comparison')\n",
    "axes[0, 1].set_ylabel('F1 Score')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].bar(models, coarse_acc, color=colors, alpha=0.7)\n",
    "axes[1, 0].set_title('Coarse Accuracy Comparison')\n",
    "axes[1, 0].set_ylabel('Accuracy')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].bar(models, coarse_f1, color=colors, alpha=0.7)\n",
    "axes[1, 1].set_title('Coarse F1 Score Comparison')\n",
    "axes[1, 1].set_ylabel('F1 Score')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# OOD Detection Analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OUT-OF-DISTRIBUTION DETECTION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def get_msp_scores(logits):\n",
    "    \"\"\"Get Maximum Softmax Probability scores.\"\"\"\n",
    "    softmax_probs = tf.nn.softmax(logits, axis=1).numpy()\n",
    "    return np.max(softmax_probs, axis=1)\n",
    "\n",
    "# Analyze OOD detection for each model\n",
    "ood_results = {}\n",
    "for model_key, predictions in all_predictions.items():\n",
    "    model_name = MODELS_CONFIG[model_key]['name']\n",
    "    \n",
    "    id_msp_scores = get_msp_scores(predictions['id_logits_h1'])\n",
    "    ood_msp_scores = get_msp_scores(predictions['ood_logits_h1'])\n",
    "    \n",
    "    # Calculate AUROC\n",
    "    labels_id = np.ones_like(id_msp_scores)\n",
    "    labels_ood = np.zeros_like(ood_msp_scores)\n",
    "    all_scores = np.concatenate([id_msp_scores, ood_msp_scores])\n",
    "    all_labels = np.concatenate([labels_id, labels_ood])\n",
    "    \n",
    "    auroc = roc_auc_score(all_labels, all_scores)\n",
    "    ood_results[model_name] = auroc\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  OOD Detection AUROC: {auroc:.4f}\")\n",
    "    print(f\"  ID MSP Mean: {np.mean(id_msp_scores):.4f}\")\n",
    "    print(f\"  OOD MSP Mean: {np.mean(ood_msp_scores):.4f}\")\n",
    "\n",
    "# Plot OOD detection comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot MSP distributions for each model\n",
    "for i, (model_key, predictions) in enumerate(all_predictions.items()):\n",
    "    model_name = MODELS_CONFIG[model_key]['name']\n",
    "    color = MODELS_CONFIG[model_key]['color']\n",
    "    \n",
    "    id_msp_scores = get_msp_scores(predictions['id_logits_h1'])\n",
    "    ood_msp_scores = get_msp_scores(predictions['ood_logits_h1'])\n",
    "    \n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.hist(id_msp_scores, bins=30, alpha=0.7, label='ID', color='blue', density=True)\n",
    "    plt.hist(ood_msp_scores, bins=30, alpha=0.7, label='OOD', color='red', density=True)\n",
    "    plt.title(f'{model_name}\\nAUROC: {ood_results[model_name]:.4f}')\n",
    "    plt.xlabel('Maximum Softmax Probability')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary of OOD detection\n",
    "print(f\"\\nðŸŽ¯ OOD DETECTION SUMMARY:\")\n",
    "best_ood_model = max(ood_results.items(), key=lambda x: x[1])\n",
    "print(f\"Best OOD Detection: {best_ood_model[0]} (AUROC: {best_ood_model[1]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18b4750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OOD] Need both ID (fine label != -1) and OOD (fine label == -1) samples in the test split.\n"
     ]
    }
   ],
   "source": [
    "# Save comprehensive results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path(\"./outputs/model_evaluation_comparison\")\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Save comparison table\n",
    "comparison_df.to_csv(output_dir / \"model_comparison_table.csv\", index=False)\n",
    "print(f\"âœ“ Saved comparison table to: {output_dir / 'model_comparison_table.csv'}\")\n",
    "\n",
    "# Save detailed metrics\n",
    "with open(output_dir / \"detailed_metrics.json\", \"w\") as f:\n",
    "    json.dump(all_metrics, f, indent=2)\n",
    "print(f\"âœ“ Saved detailed metrics to: {output_dir / 'detailed_metrics.json'}\")\n",
    "\n",
    "# Save OOD results\n",
    "with open(output_dir / \"ood_detection_results.json\", \"w\") as f:\n",
    "    json.dump(ood_results, f, indent=2)\n",
    "print(f\"âœ“ Saved OOD detection results to: {output_dir / 'ood_detection_results.json'}\")\n",
    "\n",
    "# Create summary report\n",
    "summary_report = f\"\"\"\n",
    "# Model Evaluation Summary Report\n",
    "\n",
    "## Overview\n",
    "This report compares the performance of multiple models for dermatology classification:\n",
    "- Baseline EfficientNetB1\n",
    "- SSL Fine-tuned Model\n",
    "- Voting Ensemble\n",
    "- Weighted Ensemble\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "### Best Performing Models\n",
    "- **Best Fine-grained Accuracy**: {best_fine_acc['Model']} ({best_fine_acc['Fine Accuracy']:.4f})\n",
    "- **Best Fine-grained F1**: {best_fine_f1['Model']} ({best_fine_f1['Fine F1']:.4f})\n",
    "- **Best Coarse Accuracy**: {best_coarse_acc['Model']} ({best_coarse_acc['Coarse Accuracy']:.4f})\n",
    "- **Best Coarse F1**: {best_coarse_f1['Model']} ({best_coarse_f1['Coarse F1']:.4f})\n",
    "\n",
    "### OOD Detection Performance\n",
    "- **Best OOD Detection**: {best_ood_model[0]} (AUROC: {best_ood_model[1]:.4f})\n",
    "\n",
    "## Detailed Metrics\n",
    "{comparison_df.to_string(index=False)}\n",
    "\n",
    "## Conclusions\n",
    "1. **Ensemble methods** generally show improved performance over individual models\n",
    "2. **SSL fine-tuning** demonstrates benefits of self-supervised pre-training\n",
    "3. **OOD detection** varies significantly between models\n",
    "4. **Coarse classification** tends to be more stable than fine-grained classification\n",
    "\n",
    "---\n",
    "Generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\"\"\"\n",
    "\n",
    "with open(output_dir / \"summary_report.md\", \"w\") as f:\n",
    "    f.write(summary_report)\n",
    "print(f\"âœ“ Saved summary report to: {output_dir / 'summary_report.md'}\")\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Model evaluation completed successfully!\")\n",
    "print(f\"ðŸ“ All results saved to: {output_dir}\")\n",
    "print(f\"ðŸ“Š Evaluated {len(all_predictions)} models\")\n",
    "print(f\"ðŸ“ˆ Generated comprehensive comparison analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef27e0bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31228\\932575730.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mz_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mz_fine\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mz_id\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_fine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mz_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mz_id\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_DX_CLASSES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\msi\\anaconda3\\envs\\tfm\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\msi\\anaconda3\\envs\\tfm\\Lib\\site-packages\\keras\\src\\layers\\layer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    975\u001b[0m                     \u001b[1;34m\"layers will not see the mask.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m                 )\n\u001b[0;32m    977\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    978\u001b[0m             \u001b[1;31m# Destroy call context if we created it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 979\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_reset_call_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    980\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m         \u001b[1;31m################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    982\u001b[0m         \u001b[1;31m# 8. Add a node in the graph for symbolic calls.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\msi\\anaconda3\\envs\\tfm\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\msi\\anaconda3\\envs\\tfm\\Lib\\site-packages\\keras\\src\\ops\\operation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m             call_fn = traceback_utils.inject_argument_info_in_traceback(\n\u001b[0;32m     56\u001b[0m                 \u001b[0mcall_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m                 \u001b[0mobject_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.__class__.__name__}.call()\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             )\n\u001b[1;32m---> 59\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;31m# Plain flow.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0many_symbolic_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\msi\\anaconda3\\envs\\tfm\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    213\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\msi\\anaconda3\\envs\\tfm\\Lib\\site-packages\\keras\\src\\models\\functional.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, inputs, training, mask, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[0mmasks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m                     \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_keras_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m         outputs = self._run_through_graph(\n\u001b[0m\u001b[0;32m    184\u001b[0m             \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m             operation_fn=lambda op: operation_fn(\n\u001b[0;32m    186\u001b[0m                 \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\msi\\anaconda3\\envs\\tfm\\Lib\\site-packages\\keras\\src\\ops\\function.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, inputs, operation_fn, call_fn)\u001b[0m\n\u001b[0;32m    202\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m                     \u001b[1;31m# Use NNX operation mapping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m                     \u001b[0moperation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_operation_for_node\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m                     \u001b[0mop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moperation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m                 \u001b[1;31m# Update tensor_dict.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\msi\\anaconda3\\envs\\tfm\\Lib\\site-packages\\keras\\src\\models\\functional.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[1;32mand\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             ):\n\u001b[0;32m    642\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 644\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0moperation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\msi\\anaconda3\\envs\\tfm\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\msi\\anaconda3\\envs\\tfm\\Lib\\site-packages\\keras\\src\\layers\\layer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    975\u001b[0m                     \u001b[1;34m\"layers will not see the mask.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m                 )\n\u001b[0;32m    977\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    978\u001b[0m             \u001b[1;31m# Destroy call context if we created it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 979\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_reset_call_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    980\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m         \u001b[1;31m################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    982\u001b[0m         \u001b[1;31m# 8. Add a node in the graph for symbolic calls.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\msi\\anaconda3\\envs\\tfm\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\msi\\anaconda3\\envs\\tfm\\Lib\\site-packages\\keras\\src\\ops\\operation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m             call_fn = traceback_utils.inject_argument_info_in_traceback(\n\u001b[0;32m     56\u001b[0m                 \u001b[0mcall_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m                 \u001b[0mobject_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.__class__.__name__}.call()\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             )\n\u001b[1;32m---> 59\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;31m# Plain flow.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0many_symbolic_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\msi\\anaconda3\\envs\\tfm\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    213\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\msi\\anaconda3\\envs\\tfm\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    273\u001b[0m             \u001b[0mbeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m             \u001b[0mbeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m         outputs = ops.batch_normalization(\n\u001b[0m\u001b[0;32m    278\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m             \u001b[0mmean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m             \u001b[0mvariance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvariance\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\msi\\anaconda3\\envs\\tfm\\Lib\\site-packages\\keras\\src\\ops\\nn.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, mean, variance, axis, offset, scale, epsilon)\u001b[0m\n\u001b[0;32m   2263\u001b[0m         return BatchNorm(axis, epsilon).symbolic_call(\n\u001b[0;32m   2264\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2265\u001b[0m         )\n\u001b[0;32m   2266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2267\u001b[1;33m     return backend.nn.batch_normalization(\n\u001b[0m\u001b[0;32m   2268\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2269\u001b[0m     )\n",
      "\u001b[1;32mc:\\Users\\msi\\anaconda3\\envs\\tfm\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, mean, variance, axis, offset, scale, epsilon)\u001b[0m\n\u001b[0;32m    875\u001b[0m             \u001b[0moffset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mscale\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    877\u001b[0m             \u001b[0mscale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    878\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 879\u001b[1;33m     return tf.nn.batch_normalization(\n\u001b[0m\u001b[0;32m    880\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    881\u001b[0m         \u001b[0mmean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m         \u001b[0mvariance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvariance\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\msi\\anaconda3\\envs\\tfm\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\msi\\anaconda3\\envs\\tfm\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1263\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1264\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1265\u001b[1;33m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1266\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1267\u001b[0m         \u001b[1;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1268\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\msi\\anaconda3\\envs\\tfm\\Lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, mean, variance, offset, scale, variance_epsilon, name)\u001b[0m\n\u001b[0;32m   1483\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mscale\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1484\u001b[0m       \u001b[0minv\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1485\u001b[0m     \u001b[1;31m# Note: tensorflow/contrib/quantize/python/fold_batch_norms.py depends on\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1486\u001b[0m     \u001b[1;31m# the precise order of ops that are generated by the expression below.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1487\u001b[1;33m     return x * math_ops.cast(inv, x.dtype) + math_ops.cast(\n\u001b[0m\u001b[0;32m   1488\u001b[0m         offset - mean * inv if offset is not None else -mean * inv, x.dtype)\n",
      "\u001b[1;32mc:\\Users\\msi\\anaconda3\\envs\\tfm\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\msi\\anaconda3\\envs\\tfm\\Lib\\site-packages\\tensorflow\\python\\framework\\override_binary_operator.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;31m# TODO(b/178860388): Figure out why binary_op_wrapper and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;31m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaybe_promote_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m         \u001b[1;31m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;31m# object that can implement the operator with knowledge of itself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;31m# and the tensor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\msi\\anaconda3\\envs\\tfm\\Lib\\site-packages\\tensorflow\\python\\ops\\tensor_math_operator_overrides.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_add_dispatch_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\msi\\anaconda3\\envs\\tfm\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\msi\\anaconda3\\envs\\tfm\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1263\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1264\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1265\u001b[1;33m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1266\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1267\u001b[0m         \u001b[1;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1268\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\msi\\anaconda3\\envs\\tfm\\Lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1735\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype_hint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1736\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1738\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1739\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\msi\\anaconda3\\envs\\tfm\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    478\u001b[0m         _ctx, \"AddV2\", name, x, y)\n\u001b[0;32m    479\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m       return add_v2_eager_fallback(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Additional analysis: Confusion matrices for best models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONFUSION MATRICES FOR BEST MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Plot confusion matrices for the best performing models\n",
    "best_models = {\n",
    "    'Fine Accuracy': best_fine_acc['Model'],\n",
    "    'Fine F1': best_fine_f1['Model'], \n",
    "    'Coarse Accuracy': best_coarse_acc['Model'],\n",
    "    'Coarse F1': best_coarse_f1['Model']\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "for i, (metric_name, model_name) in enumerate(best_models.items()):\n",
    "    # Find the model key for this model name\n",
    "    model_key = None\n",
    "    for key, config in MODELS_CONFIG.items():\n",
    "        if config['name'] == model_name:\n",
    "            model_key = key\n",
    "            break\n",
    "    \n",
    "    if model_key and model_key in all_predictions:\n",
    "        predictions = all_predictions[model_key]\n",
    "        \n",
    "        if 'Fine' in metric_name:\n",
    "            # Fine-grained confusion matrix\n",
    "            labels = predictions['id_labels_h1']\n",
    "            preds = np.argmax(predictions['id_logits_h1'], axis=1)\n",
    "            valid_mask = labels >= 0\n",
    "            valid_labels = labels[valid_mask]\n",
    "            valid_preds = preds[valid_mask]\n",
    "            \n",
    "            if len(valid_labels) > 0:\n",
    "                cm = confusion_matrix(valid_labels, valid_preds)\n",
    "                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                           xticklabels=DX_CLASSES, yticklabels=DX_CLASSES, ax=axes[i//2, i%2])\n",
    "                axes[i//2, i%2].set_title(f'{model_name}\\nFine-grained Confusion Matrix')\n",
    "                axes[i//2, i%2].set_ylabel('Actual')\n",
    "                axes[i//2, i%2].set_xlabel('Predicted')\n",
    "        else:\n",
    "            # Coarse confusion matrix\n",
    "            labels = predictions['id_labels_h2']\n",
    "            preds = np.argmax(predictions['id_logits_h2'], axis=1)\n",
    "            \n",
    "            cm = confusion_matrix(labels, preds)\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                       xticklabels=LESION_TYPE_CLASSES, yticklabels=LESION_TYPE_CLASSES, ax=axes[i//2, i%2])\n",
    "            axes[i//2, i%2].set_title(f'{model_name}\\nCoarse Confusion Matrix')\n",
    "            axes[i//2, i%2].set_ylabel('Actual')\n",
    "            axes[i//2, i%2].set_xlabel('Predicted')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Generated confusion matrices for best performing models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301d003a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head1_idx\n",
      "NaN     26698\n",
      "0.0     20468\n",
      "3.0      8453\n",
      "1.0      6165\n",
      "2.0      4142\n",
      "6.0      3082\n",
      "10.0     1750\n",
      "8.0       389\n",
      "7.0       386\n",
      "9.0       182\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Final summary and recommendations\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY AND RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nðŸ“Š MODEL PERFORMANCE RANKING:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Rank models by overall performance (average of all metrics)\n",
    "overall_scores = {}\n",
    "for model_key, metrics in all_metrics.items():\n",
    "    model_name = MODELS_CONFIG[model_key]['name']\n",
    "    overall_score = (\n",
    "        metrics['fine_accuracy'] + metrics['fine_f1'] + \n",
    "        metrics['coarse_accuracy'] + metrics['coarse_f1']\n",
    "    ) / 4\n",
    "    overall_scores[model_name] = overall_score\n",
    "\n",
    "# Sort by overall performance\n",
    "ranked_models = sorted(overall_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i, (model_name, score) in enumerate(ranked_models, 1):\n",
    "    print(f\"{i}. {model_name}: {score:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ KEY INSIGHTS:\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# Analyze improvements\n",
    "if 'baseline' in all_metrics:\n",
    "    baseline_score = overall_scores[MODELS_CONFIG['baseline']['name']]\n",
    "    best_score = ranked_models[0][1]\n",
    "    improvement = (best_score - baseline_score) / baseline_score * 100\n",
    "    \n",
    "    print(f\"â€¢ Best model improves over baseline by {improvement:.2f}%\")\n",
    "    \n",
    "    # Check if ensemble is best\n",
    "    ensemble_models_in_top = [name for name, _ in ranked_models[:2] if 'Ensemble' in name]\n",
    "    if ensemble_models_in_top:\n",
    "        print(f\"â€¢ Ensemble methods show superior performance\")\n",
    "    \n",
    "    # Check if SSL is beneficial\n",
    "    ssl_models = [name for name, _ in ranked_models if 'SSL' in name]\n",
    "    if ssl_models:\n",
    "        print(f\"â€¢ SSL fine-tuning demonstrates clear benefits\")\n",
    "\n",
    "print(f\"â€¢ OOD detection varies significantly between models\")\n",
    "print(f\"â€¢ Coarse classification is more stable than fine-grained\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ RECOMMENDATIONS:\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"1. **Production Model**: Use {ranked_models[0][0]} for best overall performance\")\n",
    "print(f\"2. **Ensemble Strategy**: Consider ensemble methods for critical applications\")\n",
    "print(f\"3. **SSL Pre-training**: Implement SSL for improved generalization\")\n",
    "print(f\"4. **OOD Detection**: Use {best_ood_model[0]} for uncertainty estimation\")\n",
    "print(f\"5. **Monitoring**: Track both fine-grained and coarse performance\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ NEXT STEPS:\")\n",
    "print(\"-\" * 15)\n",
    "print(f\"â€¢ Implement best model in production\")\n",
    "print(f\"â€¢ Set up continuous monitoring\")\n",
    "print(f\"â€¢ Collect more diverse training data\")\n",
    "print(f\"â€¢ Experiment with additional ensemble methods\")\n",
    "print(f\"â€¢ Investigate failure cases for improvement\")\n",
    "\n",
    "print(f\"\\nâœ… Evaluation completed successfully!\")\n",
    "print(f\"ðŸ“ Results saved to: {output_dir}\")\n",
    "print(f\"ðŸ“Š Total models evaluated: {len(all_predictions)}\")\n",
    "print(f\"ðŸŽ¯ Best overall model: {ranked_models[0][0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
